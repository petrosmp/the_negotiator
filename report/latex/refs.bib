@book{osborne,
  title={An Introduction to Game Theory},
  author={Osborne, M.J.},
  isbn={9780195681581},
  lccn={2003042981},
  url={https://books.google.com/books?id=m4yMcgAACAAJ},
  year={2004},
  publisher={Oxford University Press},
  note={(Ελληνική Μετάφραση Φ. Σκουλαρίκης, Επιμέλεια Ι. Ρεφενίδης, Εκδόσεις Κλειδάριθμος)}
}

@misc{gehalk_notes,
  author      = {Chalkiadakis, G.},
  title       = {Πολυπρακτορικά Συστήματα - Διάλεξη 3},
  year        = {2023},
  note        = {Σχολή ΗΜΜΥ Πολυτεχνείου Κρήτης},
}

@unpublished{gehalk_book,
  author = {G. Chalkiadakis and P. Stone and K. Tuyls},
  title = {Learning in Multiagent Worlds},
  year = {2020},
  month  = {September},
  note = {Unpublished Draft},
  publisher = {Morgan and Claypool Publishers}
}

@article{rabin98,
 ISSN = {00220515},
 URL = {http://www.jstor.org/stable/2564950},
 author = {M. Rabin},
 journal = {Journal of Economic Literature},
 number = {1},
 pages = {11--46},
 publisher = {American Economic Association},
 title = {Psychology and Economics},
 urldate = {2023-12-05},
 volume = {36},
 year = {1998}
}

@article{hotelling29,
 ISSN = {00130133, 14680297},
 URL = {http://www.jstor.org/stable/2224214},
 author = {H. Hotelling},
 journal = {The Economic Journal},
 number = {153},
 pages = {41--57},
 publisher = {[Royal Economic Society, Wiley]},
 title = {Stability in Competition},
 urldate = {2023-12-05},
 volume = {39},
 year = {1929}
}

@article{meta_agent_paper,
  author = {Ilany, Litan and Gal, Ya'akov},
  title = {Algorithm selection in bilateral negotiation},
  year = {2016},
  issue_date = {July 2016},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {30},
  number = {4},
  issn = {1387-2532},
  url = {https://doi.org/10.1007/s10458-015-9302-8},
  doi = {10.1007/s10458-015-9302-8},
  abstract = {Despite the abundance of strategies in the multi-agent systems literature on repeated negotiation under incomplete information, there is no single negotiation strategy that is optimal for all possible domains. Thus, agent designers face an "algorithm selection" problem--which negotiation strategy to choose when facing a new domain and unknown opponent. Our approach to this problem is to design a "meta-agent" that predicts the performance of different negotiation strategies at run-time. We study two types of the algorithm selection problem in negotiation: In the off-line variant, an agent needs to select a negotiation strategy for a given domain but cannot switch to a different strategy once the negotiation has begun. For this case, we use supervised learning to select a negotiation strategy for a new domain that is based on predicting its performance using structural features of the domain. In the on-line variant, an agent is allowed to adapt its negotiation strategy over time. For this case, we used multi-armed bandit techniques that balance the exploration---exploitation tradeoff of different negotiation strategies. Our approach was evaluated using the GENIUS negotiation test-bed that is used for the annual international Automated Negotiation Agent Competition which represents the chief venue for evaluating the state-of-the-art multi-agent negotiation strategies. We ran extensive simulations using the test bed with all of the top-contenders from both off-line and on-line negotiation tracks of the competition. The results show that the meta-agent was able to outperform all of the finalists that were submitted to the most recent competition, and to choose the best possible agent (in retrospect) for more settings than any of the other finalists. This result was consistent for both off-line and on-line variants of the algorithm selection problem. This work has important insights for multi-agent systems designers, demonstrating that "a little learning goes a long way", despite the inherent uncertainty associated with negotiation under incomplete information.},
  journal = {Autonomous Agents and Multi-Agent Systems},
  month = {jul},
  pages = {697–723},
  numpages = {27},
  keywords = {Algorithm selection, Empirical methods, GENIUS framework, Multi-agent negotiation under incomplete information}
}

@article{original_alg_selection_paper_1975,
  title={The Algorithm Selection Problem},
  author={John R. Rice},
  journal={Adv. Comput.},
  year={1976},
  volume={15},
  pages={65-118},
  url={https://api.semanticscholar.org/CorpusID:7554653}
}

@inproceedings{ANAC,
author = {Jonker, Catholijn M. and Aydo\u{g}an, Reyhan and Baarslag, Tim and Fujita, Katsuhide and Ito, Takayuki and Hindiks, Koen},
title = {Automated negotiating agents competition (ANAC)},
year = {2017},
publisher = {AAAI Press},
abstract = {The annual International Automated Negotiating Agents Competition (ANAC) is used by the automated negotiation research community to benchmark and evaluate its work and to challenge itself. The benchmark problems and evaluation results and the protocols and strategies developed are available to the wider research community.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {5070–5072},
numpages = {3},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inbook{BOA_paper,
author = {Baarslag, Tim and Hindriks, Koen and Hendrikx, Mark and Dirkzwager, Alex and Jonker, Catholijn},
year = {2014},
month = {01},
pages = {61-83},
title = {Decoupling Negotiating Agents to Explore the Space of Negotiation Strategies},
volume = {535},
isbn = {9784431547570},
journal = {Studies in Computational Intelligence},
doi = {10.1007/978-4-431-54758-7_4}
}

@Inbook{HardHeaded,
author="van Krimpen, Thijs
and Looije, Daphne
and Hajizadeh, Siamak",
editor="Ito, Takayuki
and Zhang, Minjie
and Robu, Valentin
and Matsuo, Tokuro",
title="HardHeaded",
bookTitle="Complex Automated Negotiations: Theories, Models, and Software Competitions",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="223--227",
abstract="In this paper the strategy of Hardheaded negotiating agent is described. Our agent won the Automated Negotiating Agents Competition 2011. As the name implies, the agent is hardheaded, it will not concede until the very end. Using a concession function, it generates bids in a monotonic way, which resets to a random value after the dynamic concession limit is reached. In practice, this means that most of the time the agent will cycle through the same range of bids. Since the preferences of the opponent are not known, the agent tries to learn the opponent's preference profile. It chooses bids which it thinks are optimal for the opponent in case there are equivalent bids for itself.",
isbn="978-3-642-30737-9",
doi="10.1007/978-3-642-30737-9_17",
url="https://doi.org/10.1007/978-3-642-30737-9_17"
}


@article{UCB_paper,
author = {Auer, Peter},
title = {Using confidence bounds for exploitation-exploration trade-offs},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
issn = {1532-4435},
abstract = {We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {397–422},
numpages = {26},
keywords = {reinforcement learning, online Learning, linear value function, exploitation-exploration, bandit problem}
}

@article{MAB_Lai_Robbins_1985,
title = {Asymptotically efficient adaptive allocation rules},
journal = {Advances in Applied Mathematics},
volume = {6},
number = {1},
pages = {4-22},
year = {1985},
issn = {0196-8858},
doi = {https://doi.org/10.1016/0196-8858(85)90002-8},
url = {https://www.sciencedirect.com/science/article/pii/0196885885900028},
author = {T.L Lai and Herbert Robbins}
}

@article{MAB_paper_2002,
author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Fischer, Paul},
title = {Finite-time Analysis of the Multiarmed Bandit Problem},
year = {2002},
issue_date = {May-June 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1013689704352},
doi = {10.1023/A:1013689704352},
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
journal = {Mach. Learn.},
month = {may},
pages = {235–256},
numpages = {22},
keywords = {finite horizon regret, bandit problems, adaptive allocation rules}
}